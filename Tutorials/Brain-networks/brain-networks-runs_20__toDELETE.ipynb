{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import powerlaw \n",
    "import netgraph\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from Qommunity.samplers.hierarchical.advantage_sampler import AdvantageSampler\n",
    "from Qommunity.samplers.regular.leiden_sampler import LeidenSampler\n",
    "from Qommunity.samplers.regular.louvain_sampler import LouvainSampler\n",
    "#from Qommunity.samplers.regular.bayan_sampler import BayanSampler\n",
    "\n",
    "from Qommunity.searchers.community_searcher.community_searcher import CommunitySearcher\n",
    "from Qommunity.searchers.hierarchical_community_searcher import HierarchicalCommunitySearcher\n",
    "\n",
    "from iterative_searcher.iterative_searcher import IterativeSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFromCSV:\n",
    "    \"\"\"\n",
    "    It creates an object with different properties from a csv. Everything related to it, will be\n",
    "        saved with the provided 'name' followed by the proper extensions.\n",
    "    As of now, this is not the latest version of this class (or python script), but for this pipeline is enough.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph, name, base_dir='/'):\n",
    "        self.graph = graph  # Graph file name\n",
    "        self.conns = pd.read_csv(graph, delimiter=',', header=None).values # Graph connections\n",
    "        self.graph_size = self.conns.shape\n",
    "            \n",
    "        self.name = name\n",
    "        if base_dir == '/':\n",
    "            self.dir = os.getcwd()+'/'\n",
    "        else:\n",
    "            self.dir = base_dir\n",
    "        self.originals = self.conns\n",
    "        # TODO: Add check path for base_dir!\n",
    "\n",
    "    def __revert(self):\n",
    "        \"\"\"\n",
    "        Reorders AAL3 regions by hemispheres.\n",
    "        Odd indices correspond to Left hemisphere regions.\n",
    "        Even indices correspond to rigth hemisphere regions.\n",
    "        Stores a dictionary with the reodering of indices.\n",
    "        \"\"\"\n",
    "        odd_odd = self.conns[::2, ::2]\n",
    "        odd_even = self.conns[::2, 1::2]\n",
    "        first = np.vstack((odd_odd, odd_even))\n",
    "        even_odd = self.conns[1::2, ::2]\n",
    "        even_even= self.conns[1::2, 1::2]\n",
    "        second = np.vstack((even_odd, even_even))\n",
    "        self.conns = np.hstack((first,second))\n",
    "\n",
    "        # To map actual labels with original ones\n",
    "        labels = np.array([x for x in range(0, self.graph_size[0])])\n",
    "        left = np.array([x for x in range(1, self.graph_size[0], 2)])\n",
    "        rigth = np.array([x for x in range(0, self.graph_size[0], 2)])\n",
    "        self.hemis = dict(zip(labels, np.concatenate((left, rigth), axis=0)))\n",
    "\n",
    "    def __take_log(self):\n",
    "        \"\"\"\n",
    "        Takes the natural logarithm of the connections. Enhances visualisation of the matrix.\n",
    "        \"\"\"\n",
    "        self.conns = np.log1p(self.conns)\n",
    "\n",
    "    def __plot_graph(self, save=True, show=False, fig_size=(20,15), dpi=500):\n",
    "        \"\"\"\n",
    "        Plot a graph. It assumes that the adjancency matrix is a csv file.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=fig_size)\n",
    "        plt.imshow(self.conns)\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('Connection Strength', rotation=270)\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(self.dir+self.name+'.svg', format='svg', dpi=dpi)      \n",
    "        if show:\n",
    "            plt.show()     \n",
    "\n",
    "    def process_graph(self, log=True, reshuffle=True, save=True, show=False, fig_size=(20,15)):\n",
    "        \"\"\"\n",
    "        Applies default operations to the graph to work with it.\n",
    "        \"\"\"\n",
    "        self.processed = True # The object has been processed\n",
    "        if self.conns.shape[0] <= 1:\n",
    "            raise ValueError(\"You are trying to process a flat graph. Can't do it. Unflatten your graph and set it to default.\")\n",
    "        else:\n",
    "            if log:\n",
    "                self.__take_log()\n",
    "            if reshuffle:\n",
    "                self.__revert()\n",
    "            self.__plot_graph(save=save, show=show, fig_size=(20,15))\n",
    "    \n",
    "    def get_connections(self, ini=False):\n",
    "        if not ini:\n",
    "            return self.conns \n",
    "        else:\n",
    "            return self.originals\n",
    "    \n",
    "    def flatten_graph(self, save=True):\n",
    "        \"\"\"\n",
    "        Flatten the lower triangular adjancency matrix of the graph. \n",
    "        The flattened graph becomes available after applying this method.\n",
    "        \"\"\"\n",
    "        x = self.conns.shape[0] # Dimensions of the graph \n",
    "        if x <= 1:\n",
    "            raise ValueError(\"Dimension of the graph is 1 (or lower). You can't flattened an already flattened graph\")\n",
    "        else:\n",
    "            dims = int(self.conns.shape[0]*(self.conns.shape[0]-1)/2)\n",
    "            self.flat_conns = np.zeros((1,dims))\n",
    "            k = 0\n",
    "            for i in range(x):\n",
    "                for j in range(i):\n",
    "                    self.flat_conns[0,k] = self.conns[i,j]\n",
    "                    k += 1\n",
    "            if save:\n",
    "                np.savetxt(self.dir+self.name+'_flatCM.csv', self.flat_conns, delimiter=',')\n",
    "            return self.flat_conns\n",
    "\n",
    "    def unflatten_graph(self, to_default=False):\n",
    "        \"\"\"\n",
    "        Unflatten a graph and transform it to a square symmetric matrix. \n",
    "        The unflattened graph becomes available after applying this method.\n",
    "        to_default: bool - The unflattened matrix becomes the default graph and replaces \n",
    "            the initial flat graph. As a checkpoint, the flattened graph is saved in the directory(default: False)\n",
    "        \"\"\"\n",
    "        x = self.conns.shape[0] # First dimension of the flattened graph \n",
    "        flat_dim = self.conns.shape[1]\n",
    "        if x > 1:\n",
    "            raise ValueError(\"Dimension of the graph greater than 1. You can't unflattened an already unflattened graph\")\n",
    "        else:\n",
    "            dims = int(1+np.sqrt(1+8*flat_dim)/2) # Dimensions of the squared graph\n",
    "            self.unflat_conns = np.zeros((dims,dims))\n",
    "            k = 0\n",
    "            for i in range(dims):\n",
    "                for j in range(i):\n",
    "                    self.unflat_conns[i, j] = self.conns[0, k]\n",
    "                    self.unflat_conns[j, i] = self.conns[0, k]\n",
    "                    k += 1\n",
    "            if to_default:\n",
    "                # We save the flat graph with another name\n",
    "                np.savetxt(self.dir+self.name+'_flatCM.csv', self.conns, delimiter=',')\n",
    "                # We replace the original file with the unflattend graph\n",
    "                np.savetxt(self.graph, self.unflat_conns, delimiter=',')\n",
    "                # We re-initialize the graph with the unflattened graph and both the same name and directory\n",
    "                self.__init__(self.graph, self.name, self.dir)\n",
    "            return self.unflat_conns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_process = GraphFromCSV(\"Brain-networks/control/ses-preop/sub-CON10_ses-preop_flatCM.csv\", name=\"sub-CON10_ses-preop\", base_dir=\"Brain-networks/\")\n",
    "network = G_process.unflatten_graph(to_default=False)\n",
    "network = np.delete(network, [34,35,80,81], axis=0)\n",
    "network = np.delete(network, [34,35,80,81], axis=1)\n",
    "graph = nx.from_numpy_array(network, create_using=nx.Graph, edge_attr=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Louvain modularity: 0.6109030329878278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 76.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leiden modularity: 0.6112827841745766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 20\n",
    "resolution = 1\n",
    " \n",
    "_, ms_Louv, _ = IterativeSearcher(LouvainSampler(graph, resolution=resolution)).run(num_runs=num_runs, save_results=False)\n",
    "print(\"Louvain modularity:\", ms_Louv.max())\n",
    "_, ms_Leid, _ = IterativeSearcher(LeidenSampler(graph, resolution=resolution)).run(num_runs=num_runs, save_results=False)\n",
    "print(\"Leiden modularity:\", ms_Leid.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [40:53<00:00, 122.69s/it]\n"
     ]
    }
   ],
   "source": [
    "adv_searcher = IterativeSearcher(AdvantageSampler(graph, resolution=resolution, num_reads=100, use_clique_embedding=True))\n",
    "results_adv = adv_searcher.run_with_sampleset_info(num_runs=num_runs, save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"adv-brain_network-runs_20.npy\", results_adv)\n",
    "# Moved the file to a new folder\n",
    "# np.load(\"/Brain-networks-results/adv-brain_network-runs_20.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_Adv = np.array([results_adv[i][1] for i in range(num_runs)])\n",
    "communities, division_tree, division_modularities = results_adv[ms_Adv.argmax()][0], results_adv[ms_Adv.argmax()][3], results_adv[ms_Adv.argmax()][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = results_adv.communities\n",
    "modularities = results_adv.modularity\n",
    "division_trees = results_adv.division_tree\n",
    "division_modularities = results_adv.division_modularities\n",
    "times = results_adv.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_modularity_increments(division_modularities: list[float]) -> list[float]:\n",
    "    mod_increments = [\n",
    "        division_modularities[i + 1] - division_modularities[i]\n",
    "        for i in range(0, len(division_modularities) - 1)\n",
    "    ]\n",
    "    return mod_increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: {str(message)}\n"
     ]
    }
   ],
   "source": [
    "modularity_increments = []\n",
    "for div_mod in division_modularities:\n",
    "    mod_increments = calc_modularity_increments(div_mod)\n",
    "    modularity_increments.append(np.array(mod_increments))\n",
    "\n",
    "modularity_increments = np.array(modularity_increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.15150293e-01,  1.53329831e-01,  1.63627253e-02,  1.10710208e-03,\n",
       "        -1.77781720e-04, -3.22024096e-04,  4.70025893e-08]),\n",
       " array([ 4.16131021e-01,  1.57058913e-01,  1.47968473e-02,  2.94989164e-04,\n",
       "        -1.22845798e-03, -3.54195073e-04, -1.08286304e-03]),\n",
       " array([ 4.18061073e-01,  1.55299385e-01,  1.62844319e-02,  7.66439524e-04,\n",
       "         2.51151918e-04, -6.98934959e-05]),\n",
       " array([ 4.23160822e-01,  1.53718547e-01,  1.49613938e-02,  1.26441015e-03,\n",
       "         1.48706309e-05, -2.20727940e-03]),\n",
       " array([ 0.42000781,  0.1582157 ,  0.01579647,  0.00103627, -0.00176747,\n",
       "        -0.00057639, -0.00043425]),\n",
       " array([ 4.19562775e-01,  1.56391689e-01,  1.52191817e-02, -1.72228914e-03,\n",
       "         3.54167623e-04, -5.74661750e-04, -2.90826735e-04]),\n",
       " array([ 4.20766689e-01,  1.57415803e-01,  1.46004669e-02, -9.02911115e-04,\n",
       "        -6.94000455e-04, -5.61437220e-06,  4.50274390e-08]),\n",
       " array([ 4.17447328e-01,  1.59257661e-01,  1.43350310e-02,  8.29910913e-04,\n",
       "         1.20550957e-04,  6.34065264e-06, -1.36727479e-03]),\n",
       " array([ 4.24905342e-01,  1.53246149e-01,  1.50342690e-02,  5.02420135e-04,\n",
       "        -2.94137993e-04, -1.59372604e-03,  1.06549513e-06]),\n",
       " array([ 4.14516752e-01,  1.61335520e-01,  1.42728848e-02,  5.39071507e-04,\n",
       "        -2.85253559e-04,  1.70497969e-06,  1.17322277e-07]),\n",
       " array([ 4.20492967e-01,  1.63329945e-01,  1.35771647e-02,  1.78953906e-04,\n",
       "        -2.69768068e-04,  6.83413659e-07])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row for row in modularity_increments if np.any(row < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True,  True, False, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vectorize(lambda row: (row < 0).any())(modularity_increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_neg_mod_increase = np.vectorize(lambda row: (row < 0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True,  True, False, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_neg_mod_increase(modularity_increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 4.15150293e-01,  1.53329831e-01,  1.63627253e-02,  1.10710208e-03,\n",
       "              -1.77781720e-04, -3.22024096e-04,  4.70025893e-08])                ,\n",
       "       array([ 4.16131021e-01,  1.57058913e-01,  1.47968473e-02,  2.94989164e-04,\n",
       "              -1.22845798e-03, -3.54195073e-04, -1.08286304e-03])                ,\n",
       "       array([ 4.18061073e-01,  1.55299385e-01,  1.62844319e-02,  7.66439524e-04,\n",
       "               2.51151918e-04, -6.98934959e-05])                                 ,\n",
       "       array([ 4.23160822e-01,  1.53718547e-01,  1.49613938e-02,  1.26441015e-03,\n",
       "               1.48706309e-05, -2.20727940e-03])                                 ,\n",
       "       array([ 0.42000781,  0.1582157 ,  0.01579647,  0.00103627, -0.00176747,\n",
       "              -0.00057639, -0.00043425])                                      ,\n",
       "       array([ 4.19562775e-01,  1.56391689e-01,  1.52191817e-02, -1.72228914e-03,\n",
       "               3.54167623e-04, -5.74661750e-04, -2.90826735e-04])                ,\n",
       "       array([ 4.20766689e-01,  1.57415803e-01,  1.46004669e-02, -9.02911115e-04,\n",
       "              -6.94000455e-04, -5.61437220e-06,  4.50274390e-08])                ,\n",
       "       array([ 4.17447328e-01,  1.59257661e-01,  1.43350310e-02,  8.29910913e-04,\n",
       "               1.20550957e-04,  6.34065264e-06, -1.36727479e-03])                ,\n",
       "       array([ 4.24905342e-01,  1.53246149e-01,  1.50342690e-02,  5.02420135e-04,\n",
       "              -2.94137993e-04, -1.59372604e-03,  1.06549513e-06])                ,\n",
       "       array([ 4.14516752e-01,  1.61335520e-01,  1.42728848e-02,  5.39071507e-04,\n",
       "              -2.85253559e-04,  1.70497969e-06,  1.17322277e-07])                ,\n",
       "       array([ 4.20492967e-01,  1.63329945e-01,  1.35771647e-02,  1.78953906e-04,\n",
       "              -2.69768068e-04,  6.83413659e-07])                                 ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modularity_increments[np.where(has_neg_mod_increase(modularity_increments))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % Of negative modlarity increment obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.00000000000001"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modularity_increments[has_neg_mod_increase(modularity_increments)].shape[0] / modularity_increments.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_Adv = np.array([results_adv[i][1] for i in range(num_runs)])\n",
    "communities, division_tree, division_modularities = results_adv[ms_Adv.argmax()][0], results_adv[ms_Adv.argmax()][3], results_adv[ms_Adv.argmax()][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum:\n",
      "communities detected no.: 21\n",
      "Modularity: 0.5998335088559781\n",
      "No. of recursive divisions (division levels): 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimum:\")\n",
    "print(f\"communities detected no.: {len(communities)}\")\n",
    "print(f\"Modularity: {division_modularities[-1]}\")\n",
    "print(f\"No. of recursive divisions (division levels): {len(division_tree)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And where negative modularity increase occured:\n",
    "\n",
    "(suboptimas, one might say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3,  4,  9, 10, 11, 12, 13, 14, 15, 18, 19], dtype=int64),)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.where(has_neg_mod_increase(modularity_increments))\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_sub = results_adv.communities[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where dicrease in modularity occured\n",
      "[26, 32, 27, 29, 25, 28, 32, 26, 27, 26, 24]\n",
      "mean of communities det. where modularity dicrease occured: 27.454545454545453\n"
     ]
    }
   ],
   "source": [
    "print(\"Where dicrease in modularity occured\")\n",
    "no_communities_detected_sub = [len(comm) for comm in comms_sub]\n",
    "print(no_communities_detected_sub)\n",
    "print(f\"mean of communities det. where modularity dicrease occured: {np.array(no_communities_detected_sub).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_complement = results_adv.communities[np.where(~has_neg_mod_increase(modularity_increments))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Standard'/'expected' behaviour - where dicrease in modularity did not occur\n",
      "[24, 27, 24, 21, 21, 28, 30, 28, 28]\n",
      "mean of communities detected: 25.666666666666668\n"
     ]
    }
   ],
   "source": [
    "print(\"'Standard'/'expected' behaviour - where dicrease in modularity did not occur\")\n",
    "no_communities_detected = [len(comm) for comm in comms_complement]\n",
    "print(no_communities_detected)\n",
    "print(f\"mean of communities detected: {np.array(no_communities_detected).mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
